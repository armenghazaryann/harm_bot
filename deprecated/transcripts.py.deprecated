"""
DEPRECATED: Custom transcript processing - COMPLETELY REPLACED BY LANGCHAIN

This entire file is obsolete and replaced by workers/langchain_processor.py
which provides superior functionality using production-ready LangChain patterns:

REPLACED BY:
- LangChain PyPDFLoader/UnstructuredPDFLoader (PDF extraction)
- LangChain RecursiveCharacterTextSplitter (optimal chunking)
- LangChain OpenAIEmbeddings + PGVector (automatic embedding storage)
- LangChain RetrievalQA chains (question answering)

CLEANUP STATUS: This file should be deleted after confirming LangChain pipeline works.
All functionality has been migrated to LangChain components.
"""
from __future__ import annotations

import asyncio
import hashlib
import io
import json
import re
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Tuple

import structlog
import alg
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.graphs import Neo4jGraph

try:  # optional tokenizer for accurate token counting
    import tiktoken  # type: ignore
except Exception:  # pragma: no cover - optional dependency
    tiktoken = None  # type: ignore

from core.settings import SETTINGS
from infra.resources import MinIOResource
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from api.features.documents.entities.document import Document as DocumentEntity
from api.features.query.entities.utterance import Utterance as UtteranceEntity
from api.features.query.entities.chunk import (
    Chunk as ChunkEntity,
    ChunkType as ChunkTypeEnum,
)

from infra.db_utils import DatabaseManager

logger = structlog.get_logger("workers.transcripts")


# --------- DB Helpers (DRY) ---------


async def _load_document(
    session: AsyncSession, doc_id: str
) -> Optional[DocumentEntity]:
    stmt = select(DocumentEntity).where(DocumentEntity.id == doc_id)
    res = await session.execute(stmt)
    return res.scalar_one_or_none()


async def _update_processing_metadata(
    session: AsyncSession, doc_id: str, updates: Dict[str, Any]
) -> None:
    ent = await _load_document(session, doc_id)
    if ent is None:
        return
    meta = ent.processing_metadata or {}
    meta.update(updates)
    ent.processing_metadata = meta
    await session.commit()


@dataclass
class Page:
    page_no: int
    text: str
    extraction_method: str = "text"


@dataclass
class Utterance:
    utterance_id: str
    doc_id: str
    turn_index: int
    speaker: str
    role: str
    speech: str
    section: str
    page_spans: List[Dict[str, Any]]
    extraction_method: str
    meta: Dict[str, Any]


# --------- Storage Keys ---------


def utterances_key(doc_id: str) -> str:
    return f"transcripts/{doc_id}/utterances.jsonl"


def report_key(doc_id: str) -> str:
    return f"transcripts/{doc_id}/report.json"


# --------- PDF Extraction ---------


def _normalize_text(text: str) -> str:
    # Merge hyphenated line breaks: word-\n continuation -> wordcontinuation
    text = re.sub(r"(\w)-\n(\w)", r"\1\2", text)
    # Replace newlines that are mid-paragraph with spaces when not followed by empty line
    text = re.sub(r"(?<!\n)\n(?!\n)", " ", text)
    # Collapse multiple spaces
    text = re.sub(r"[ \t]+", " ", text)
    # Normalize em dashes and colon spacing
    text = text.replace("—", " - ")
    text = re.sub(r"\s*:\s*", ": ", text)
    # Preserve paragraph breaks as double newline
    text = re.sub(r"\n{2,}", "\n\n", text)
    return text.strip()


def _extract_pages_with_pymupdf(pdf_bytes: bytes) -> List[Page]:
    import fitz  # type: ignore

    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages: List[Page] = []
    try:
        for i in range(doc.page_count):
            page = doc.load_page(i)
            text = page.get_text("text")
            pages.append(Page(page_no=i + 1, text=text, extraction_method="text"))
    finally:
        doc.close()
    return pages


def _extract_pages_with_pdfminer(pdf_bytes: bytes) -> List[Page]:
    from pdfminer.high_level import extract_text  # type: ignore

    # pdfminer works on file-like objects
    text = extract_text(io.BytesIO(pdf_bytes))
    # It returns full doc text; split naively by form feed (page breaks) if present
    # Many PDFs may not include FF; in that case, produce single page
    parts = text.split("\x0c") if "\x0c" in text else [text]
    pages: List[Page] = []
    for idx, part in enumerate(parts, start=1):
        pages.append(Page(page_no=idx, text=part, extraction_method="text"))
    return pages


def extract_pages(pdf_bytes: bytes) -> List[Page]:
    """Try PyMuPDF then pdfminer; no OCR in MVP."""
    try:
        return _extract_pages_with_pymupdf(pdf_bytes)
    except Exception as e:
        logger.warning("pymupdf_failed_fallback_pdfminer", error=str(e))
        try:
            return _extract_pages_with_pdfminer(pdf_bytes)
        except Exception as e2:
            logger.error("pdfminer_failed", error=str(e2))
            raise RuntimeError(
                "Failed to extract text from PDF. Install extras: pip install '.[pdf]'"
            )


# --------- Speaker Segmentation ---------

SPEAKER_LABEL_RE = re.compile(
    r"^(?P<label>(?:[A-Z][A-Za-z'\-\.]+(?:\s+[A-Z][A-Za-z'\-\.]+)*|[A-Z][A-Z ]+)(?:\s*\([^)]*\))?(?:\s*—\s*[^:]+)?)\s*:\s*(?P<rest>.*)$"
)
SECTION_HEADERS = {
    "participants": re.compile(r"^participants$", re.IGNORECASE),
    "prepared_remarks": re.compile(
        r"^(prepared\s+remarks|opening\s+remarks|presentation)$", re.IGNORECASE
    ),
    "qa": re.compile(r"^(q\s*&\s*a|question\s+and\s+answer|q\+a)$", re.IGNORECASE),
}


def _infer_role(label: str) -> Tuple[str, Dict[str, Any]]:
    lbl = label.strip()
    norm = lbl.lower()
    meta: Dict[str, Any] = {}
    if "operator" in norm:
        return "operator", meta
    # Analysts commonly have em dash with firm
    if "—" in lbl or " - " in lbl:
        parts = re.split(r"\s*[—-]\s*", lbl, maxsplit=1)
        if len(parts) == 2:
            meta["firm"] = parts[1]
        return "analyst", meta
    # Titles hint at management
    if re.search(
        r"\b(ceo|cfo|coo|chief|president|vp|officer|founder|head|director)\b", norm
    ):
        return "management", meta
    return "unknown", meta


def segment_utterances(
    pages: List[Page], doc_id: str
) -> Tuple[List[Utterance], Dict[str, Any]]:
    current_section = "other"
    turns: List[Utterance] = []
    turn_index = 0
    current_speaker = None
    current_role = "unknown"
    current_meta: Dict[str, Any] = {}
    current_text_parts: List[str] = []
    current_page_spans: List[Dict[str, Any]] = []
    # Diagnostics
    lines_total = 0
    speaker_line_matches = 0
    header_hits = {k: 0 for k in ["participants", "prepared_remarks", "qa"]}

    def flush_turn():
        nonlocal \
            turn_index, \
            current_text_parts, \
            current_page_spans, \
            current_speaker, \
            current_role, \
            current_meta
        if current_speaker and current_text_parts:
            speech = _normalize_text(" ".join(current_text_parts)).strip()
            if speech:
                uid_src = f"{doc_id}|{current_speaker}|{speech}|{turn_index}"
                uid = hashlib.sha256(uid_src.encode("utf-8")).hexdigest()
                turns.append(
                    Utterance(
                        utterance_id=uid,
                        doc_id=doc_id,
                        turn_index=turn_index,
                        speaker=current_speaker,
                        role=current_role,
                        speech=speech,
                        section=current_section,
                        page_spans=current_page_spans.copy(),
                        extraction_method="text",
                        meta=current_meta.copy(),
                    )
                )
                turn_index += 1
        # reset accumulators
        current_text_parts = []
        current_page_spans = []

    # Relaxed speaker label: allow missing colon; optional firm after dash
    RELAXED_SPEAKER_RE = re.compile(
        r"^(?P<label>(?:[A-Z][A-Za-z'\-\.]+(?:\s+[A-Z][A-Za-z'\-\.]*)*|[A-Z][A-Z ]+)(?:\s*[—-]\s*[^:]+)?)\s*:?\s*(?P<rest>.*)$"
    )

    for page in pages:
        # Cheap header/footer removal: drop extremely short lines that repeat page number patterns
        lines = [ln.strip() for ln in page.text.splitlines()]
        for i, raw in enumerate(lines):
            lines_total += 1
            if not raw:
                continue
            # Section headers
            if SECTION_HEADERS["participants"].match(raw):
                flush_turn()
                current_section = "participants"
                header_hits["participants"] += 1
                continue
            if SECTION_HEADERS["prepared_remarks"].match(raw):
                flush_turn()
                current_section = "prepared_remarks"
                header_hits["prepared_remarks"] += 1
                continue
            if SECTION_HEADERS["qa"].match(raw):
                flush_turn()
                current_section = "qa"
                header_hits["qa"] += 1
                continue

            m = SPEAKER_LABEL_RE.match(raw)
            if m:
                # Start new turn
                flush_turn()
                speaker_line_matches += 1
                label = m.group("label").strip()
                rest = m.group("rest").strip()
                role, meta = _infer_role(label)
                current_speaker = re.sub(r"\s{2,}", " ", label)
                current_role = role
                current_meta = meta
                if rest:
                    current_text_parts.append(rest)
                current_page_spans.append({"page_no": page.page_no})
            else:
                # Try relaxed speaker detection (no colon). If matches and we don't currently
                # have a speaker, start a new turn seeded with rest or next line.
                m2 = RELAXED_SPEAKER_RE.match(raw)
                if m2 and not current_speaker:
                    label = m2.group("label").strip()
                    rest = (m2.group("rest") or "").strip()
                    role, meta = _infer_role(label)
                    current_speaker = re.sub(r"\s{2,}", " ", label)
                    current_role = role
                    current_meta = meta
                    seed = rest
                    if not seed and i + 1 < len(lines):
                        nxt = lines[i + 1].strip()
                        # Seed with the next non-empty line to get speech going
                        if nxt:
                            seed = nxt
                    if seed:
                        current_text_parts.append(seed)
                    current_page_spans.append({"page_no": page.page_no})
                    speaker_line_matches += 1
                else:
                    # Continuation of current speaker or unmatched text (skip if no speaker yet)
                    if current_speaker:
                        current_text_parts.append(raw)
                        # Only add page once per contiguous block; here we append only if empty
                        if (
                            not current_page_spans
                            or current_page_spans[-1].get("page_no") != page.page_no
                        ):
                            current_page_spans.append({"page_no": page.page_no})
                    else:
                        # Unmatched lines before first speaker: ignore or collect to meta
                        pass
        # end for lines
    # end for pages

    # Final flush
    flush_turn()

    # Fallback: if we failed to segment any turns, use alg.py approach on combined text
    if not turns and pages:
        combined = "\n".join(p.text for p in pages if p.text)
        labels = alg.find_candidate_labels(combined)
        dialogues = alg.segment_by_labels(combined, labels)
        if not dialogues:
            # second fallback from alg: simple line-based
            lines = [ln.strip() for ln in combined.splitlines() if ln.strip()]
            sp = re.compile(r"^([A-Z][A-Za-z0-9\s\.,&\-]{0,80}?):\s*(.*)$")
            current_speaker = None
            current_speech: List[str] = []
            for line in lines:
                m = sp.match(line)
                if m:
                    if current_speaker:
                        speech = _normalize_text(" ".join(current_speech))
                        if speech:
                            role, meta = _infer_role(current_speaker)
                            uid_src = (
                                f"{doc_id}|{current_speaker}|{speech}|{turn_index}"
                            )
                            uid = hashlib.sha256(uid_src.encode("utf-8")).hexdigest()
                            turns.append(
                                Utterance(
                                    utterance_id=uid,
                                    doc_id=doc_id,
                                    turn_index=turn_index,
                                    speaker=current_speaker,
                                    role=role,
                                    speech=speech,
                                    section=current_section,
                                    page_spans=[],
                                    extraction_method="text",
                                    meta=meta,
                                )
                            )
                            turn_index += 1
                    current_speaker = m.group(1).strip()
                    first = (m.group(2) or "").strip()
                    current_speech = [first] if first else []
                else:
                    if current_speaker:
                        current_speech.append(line)
            if current_speaker:
                speech = _normalize_text(" ".join(current_speech))
                if speech:
                    role, meta = _infer_role(current_speaker)
                    uid_src = f"{doc_id}|{current_speaker}|{speech}|{turn_index}"
                    uid = hashlib.sha256(uid_src.encode("utf-8")).hexdigest()
                    turns.append(
                        Utterance(
                            utterance_id=uid,
                            doc_id=doc_id,
                            turn_index=turn_index,
                            speaker=current_speaker,
                            role=role,
                            speech=speech,
                            section=current_section,
                            page_spans=[],
                            extraction_method="text",
                            meta=meta,
                        )
                    )
                    turn_index += 1
        else:
            for d in dialogues:
                speaker = d.get("speaker", "").strip()
                speech = _normalize_text(d.get("speech", ""))
                if not speaker or not speech:
                    continue
                role, meta = _infer_role(speaker)
                uid_src = f"{doc_id}|{speaker}|{speech}|{turn_index}"
                uid = hashlib.sha256(uid_src.encode("utf-8")).hexdigest()
                turns.append(
                    Utterance(
                        utterance_id=uid,
                        doc_id=doc_id,
                        turn_index=turn_index,
                        speaker=speaker,
                        role=role,
                        speech=speech,
                        section=current_section,
                        page_spans=[],
                        extraction_method="text",
                        meta=meta,
                    )
                )
                turn_index += 1

    report = {
        "pages_total": len(pages),
        "turns_total": len(turns),
        "speakers_distinct": len({t.speaker for t in turns}),
        "sections": {
            s: sum(1 for t in turns if t.section == s)
            for s in ["participants", "prepared_remarks", "qa", "other"]
        },
        "extraction_method": "text",
        "diagnostics": {
            "lines_total": lines_total,
            "speaker_label_matches": speaker_line_matches,
            "section_header_hits": header_hits,
        },
    }
    return turns, report


# --------- MinIO I/O ---------


async def _get_minio() -> MinIOResource:
    minio = MinIOResource(
        endpoint=SETTINGS.MINIO.MINIO_ENDPOINT,
        access_key=SETTINGS.MINIO.MINIO_ACCESS_KEY,
        secret_key=SETTINGS.MINIO.MINIO_SECRET_KEY.get_secret_value(),
        bucket_name=SETTINGS.MINIO.MINIO_BUCKET,
    )
    await minio.init()
    return minio


# Removed: use shared get_db from workers.db_utils


# Removed: use Neo4jGraph via LangChain instead of custom Neo4jResource


async def read_pdf_from_minio(doc: DocumentEntity, minio: MinIOResource) -> bytes:
    """Download raw PDF bytes from MinIO given a Document row."""
    client = minio.client
    bucket = minio.bucket_name
    resp = client.get_object(bucket, doc.raw_path)
    try:
        data = resp.read()
        return data
    finally:
        resp.close()
        resp.release_conn()


async def write_jsonl_to_minio(
    doc_id: str, items: Iterable[Dict[str, Any]], minio: MinIOResource
) -> str:
    key = utterances_key(doc_id)
    payload = "".join(json.dumps(it, ensure_ascii=False) + "\n" for it in items).encode(
        "utf-8"
    )
    client = minio.client
    bucket = minio.bucket_name
    client.put_object(
        bucket,
        key,
        io.BytesIO(payload),
        length=len(payload),
        content_type="application/jsonl",
    )
    return key


async def write_debug_text_to_minio(
    doc_id: str, name: str, text: str, minio: MinIOResource
) -> str:
    """Write a plain text debug artifact to MinIO under transcripts/{doc_id}/{name}.txt"""
    key = f"transcripts/{doc_id}/{name}.txt"
    payload = text.encode("utf-8")
    client = minio.client
    bucket = minio.bucket_name
    client.put_object(
        bucket,
        key,
        io.BytesIO(payload),
        length=len(payload),
        content_type="text/plain; charset=utf-8",
    )
    return key


async def write_report_to_minio(
    doc_id: str, report: Dict[str, Any], minio: MinIOResource
) -> str:
    key = report_key(doc_id)
    payload = json.dumps(report, ensure_ascii=False, indent=2).encode("utf-8")
    client = minio.client
    bucket = minio.bucket_name
    client.put_object(
        bucket,
        key,
        io.BytesIO(payload),
        length=len(payload),
        content_type="application/json",
    )
    return key


# --------- Postgres Upsert ---------


async def upsert_utterances_pg(
    doc_id: str, utterances: List[Utterance], session: AsyncSession
) -> int:
    # Load existing ids to avoid duplicates
    existing_stmt = select(UtteranceEntity.utterance_id).where(
        UtteranceEntity.document_id == doc_id
    )
    res = await session.execute(existing_stmt)
    existing_ids = set(res.scalars().all())

    added = 0
    for u in utterances:
        if u.utterance_id in existing_ids:
            continue
        row = UtteranceEntity(
            document_id=doc_id,
            utterance_id=u.utterance_id,
            turn_index=u.turn_index,
            speaker=u.speaker,
            role=u.role,
            section=u.section,
            speech=u.speech,
            page_spans=u.page_spans,
            extraction_method=u.extraction_method,
        )
        session.add(row)
        added += 1
    await session.commit()
    return added


# --------- Neo4j Ingestion (via LangChain) ---------


def ingest_graph_neo4j_langchain(doc_id: str, utterances: List[Utterance]) -> int:
    """Ingest transcript graph into Neo4j using LangChain's Neo4jGraph wrapper.

    This reduces custom driver code and keeps Cypher logic concise.
    """
    try:
        graph = Neo4jGraph(
            url=SETTINGS.NEO4J.NEO4J_URI,
            username=SETTINGS.NEO4J.NEO4J_USER,
            password=SETTINGS.NEO4J.NEO4J_PASSWORD.get_secret_value(),
        )
    except Exception as e:
        if "APOC" in str(e):
            logger.warning(f"Neo4j APOC plugin not available, skipping Neo4j ingestion: {e}")
            return 0
        else:
            raise
    # Constraints and indexes
    graph.query(
        "CREATE CONSTRAINT IF NOT EXISTS FOR (c:Call) REQUIRE c.doc_id IS UNIQUE"
    )
    graph.query(
        "CREATE CONSTRAINT IF NOT EXISTS FOR (u:Utterance) REQUIRE u.utterance_id IS UNIQUE"
    )
    graph.query("CREATE INDEX IF NOT EXISTS FOR (s:Speaker) ON (s.name)")

    # Upsert Call
    graph.query(
        "MERGE (c:Call {doc_id:$doc_id}) ON CREATE SET c.source=$source",
        params={
            "doc_id": doc_id,
            "source": f"minio://{SETTINGS.MINIO.MINIO_BUCKET}/raw/{doc_id}",
        },
    )
    # Speakers roster
    speakers = {u.speaker: u for u in utterances}
    for spk, u in speakers.items():
        graph.query("MERGE (s:Speaker {name:$name})", params={"name": spk})
        firm = u.meta.get("firm")
        if firm:
            graph.query(
                "MATCH (s:Speaker {name:$name}) SET s.firm = coalesce(s.firm, $firm)",
                params={"name": spk, "firm": firm},
            )

    # Utterances + relationships
    prev_uid: Optional[str] = None
    count = 0
    for u in utterances:
        graph.query(
            "MERGE (u:Utterance {utterance_id:$uid}) ON CREATE SET u.turn_index=$turn, u.section=$section",
            params={"uid": u.utterance_id, "turn": u.turn_index, "section": u.section},
        )
        graph.query(
            "MATCH (c:Call {doc_id:$doc_id}), (u:Utterance {utterance_id:$uid}) MERGE (c)-[:CONTAINS]->(u)",
            params={"doc_id": doc_id, "uid": u.utterance_id},
        )
        graph.query(
            "MATCH (s:Speaker {name:$name}), (u:Utterance {utterance_id:$uid}) MERGE (s)-[:SPOKE]->(u)",
            params={"name": u.speaker, "uid": u.utterance_id},
        )
        if prev_uid is not None:
            graph.query(
                "MATCH (u1:Utterance {utterance_id:$u1}), (u2:Utterance {utterance_id:$u2}) MERGE (u1)-[:NEXT]->(u2)",
                params={"u1": prev_uid, "u2": u.utterance_id},
            )
        prev_uid = u.utterance_id
        count += 1
    return count


# --------- Orchestration API ---------


async def process_transcript_document(doc_id: str) -> Dict[str, Any]:
    """Deprecated: Orchestrate by calling existing steps; prefer Celery chain tasks.

    Kept for compatibility in DI; internally delegates to smaller functions.
    """
    r1 = await create_utterances_jsonl(doc_id)
    r2 = await ingest_transcript_pg_from_minio(doc_id)
    r3 = await ingest_transcript_neo4j_from_minio(doc_id)
    return {**r1, **r2, **r3}


async def create_utterances_jsonl(doc_id: str) -> Dict[str, Any]:
    """Extract + segment transcript and write MinIO artifacts only.

    Sets Document.status to CHUNKED and records artifact keys in processing_metadata.
    """
    minio, db = await asyncio.gather(_get_minio(), DatabaseManager.get_resource())

    # Load document
    async with db.get_session() as session:
        doc = await _load_document(session, doc_id)
        if not doc:
            raise RuntimeError(f"Document {doc_id} not found")
        if "transcript" not in str(getattr(doc, "doc_type", "")).lower():
            raise RuntimeError(f"Document {doc_id} is not a transcript")

    pdf_bytes = await read_pdf_from_minio(doc, minio)
    pages = extract_pages(pdf_bytes)
    for p in pages:
        p.text = _normalize_text(p.text)
    turns, report = segment_utterances(pages, doc_id)
    # If no turns, store first page text for debugging
    if not turns and pages:
        try:
            debug_key = await write_debug_text_to_minio(
                doc_id, "debug_first_page", pages[0].text, minio
            )
            report.setdefault("diagnostics", {})["debug_first_page_key"] = debug_key
            report["diagnostics"]["debug_first_page_chars"] = len(pages[0].text)
        except Exception as e:
            logger.warning("debug_first_page_write_failed", error=str(e))

    key_jsonl, key_report = await asyncio.gather(
        write_jsonl_to_minio(doc_id, [u.__dict__ for u in turns], minio),
        write_report_to_minio(doc_id, report, minio),
    )

    # Metadata update only
    async with db.get_session() as session:
        await _update_processing_metadata(
            session,
            doc_id,
            {
                "utterances_key": key_jsonl,
                "report_key": key_report,
                "utterances_count": len(turns),
            },
        )

    return {
        "doc_id": doc_id,
        "utterances_written": len(turns),
        "utterances_key": key_jsonl,
        "report_key": key_report,
    }


async def load_utterances_from_minio(
    doc_id: str, minio: MinIOResource
) -> List[Utterance]:
    """Read utterances.jsonl from MinIO and parse to Utterance dataclasses."""
    client = minio.client
    bucket = minio.bucket_name
    key = utterances_key(doc_id)
    resp = client.get_object(bucket, key)
    utterances: List[Utterance] = []
    try:
        buffer = ""
        for chunk in resp.stream(32 * 1024):  # bytes
            buffer += chunk.decode("utf-8")
            while True:
                nl = buffer.find("\n")
                if nl == -1:
                    break
                line = buffer[:nl]
                buffer = buffer[nl + 1 :]
                if not line:
                    continue
                obj = json.loads(line)
                utterances.append(
                    Utterance(
                        utterance_id=obj["utterance_id"],
                        doc_id=obj["doc_id"],
                        turn_index=obj["turn_index"],
                        speaker=obj["speaker"],
                        role=obj.get("role", "unknown"),
                        speech=obj["speech"],
                        section=obj.get("section", "other"),
                        page_spans=obj.get("page_spans", []),
                        extraction_method=obj.get("extraction_method", "text"),
                        meta=obj.get("meta", {}),
                    )
                )
        # Handle trailing buffer without newline
        tail = buffer.strip()
        if tail:
            obj = json.loads(tail)
            utterances.append(
                Utterance(
                    utterance_id=obj["utterance_id"],
                    doc_id=obj["doc_id"],
                    turn_index=obj["turn_index"],
                    speaker=obj["speaker"],
                    role=obj.get("role", "unknown"),
                    speech=obj["speech"],
                    section=obj.get("section", "other"),
                    page_spans=obj.get("page_spans", []),
                    extraction_method=obj.get("extraction_method", "text"),
                    meta=obj.get("meta", {}),
                )
            )
    finally:
        resp.close()
        resp.release_conn()
    return utterances


async def ingest_transcript_pg_from_minio(doc_id: str) -> Dict[str, Any]:
    """Load utterances.jsonl from MinIO and upsert into Postgres."""
    minio = await _get_minio()
    db = await DatabaseManager.get_resource()
    async with db.get_session() as session:
        turns = await load_utterances_from_minio(doc_id, minio)
        inserted = await upsert_utterances_pg(doc_id, turns, session)
    return {"doc_id": doc_id, "pg_inserted": inserted}


async def ingest_transcript_neo4j_from_minio(doc_id: str) -> Dict[str, Any]:
    """Load utterances.jsonl from MinIO and ingest graph into Neo4j."""
    minio = await _get_minio()
    turns = await load_utterances_from_minio(doc_id, minio)
    # Use LangChain Neo4jGraph for simpler ingestion
    ingested = ingest_graph_neo4j_langchain(doc_id, turns)
    return {"doc_id": doc_id, "neo4j_ingested": ingested}


async def materialize_transcript_chunks_from_pg(doc_id: str) -> Dict[str, Any]:
    """Create Chunk rows for embedding using token-based sliding-window chunking.

    Strategy:
    - Group consecutive utterances into chunks targeting SETTINGS.PROCESSING.CHUNK_TOKENS tokens.
    - Apply overlap of SETTINGS.PROCESSING.CHUNK_OVERLAP_TOKENS tokens between chunks.
    - For any single overlong utterance, split by sentence boundaries.
    - Preserve metadata: speakers, roles, utterance_ids, and dominant section.
    """
    db = await DatabaseManager.get_resource()
    async with db.get_session() as session:
        inserted = 0

        target_tokens = getattr(SETTINGS.PROCESSING, "CHUNK_TOKENS", 400)
        overlap_tokens = getattr(SETTINGS.PROCESSING, "CHUNK_OVERLAP_TOKENS", 80)
        # Tokenizer helpers
        enc = None
        if tiktoken is not None:
            try:
                enc = tiktoken.get_encoding("cl100k_base")
            except Exception:
                enc = None

        def count_tokens(text: str) -> int:
            if enc is not None:
                try:
                    return len(enc.encode(text))
                except Exception:
                    pass
            return max(1, len(text.split()))

        # Prepare LC splitter (token-based if possible; else character-based fallback ~4 chars per token)
        if enc is not None:
            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
                chunk_size=target_tokens,
                chunk_overlap=overlap_tokens,
                separators=["\n\n", "\n", " ", ""],
            )
        else:
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=target_tokens * 4,
                chunk_overlap=overlap_tokens * 4,
                separators=["\n\n", "\n", " ", ""],
            )

        # Load utterances for document
        u_stmt = (
            select(UtteranceEntity)
            .where(UtteranceEntity.document_id == doc_id)
            .order_by(UtteranceEntity.turn_index.asc())
        )
        res = await session.execute(u_stmt)
        utterances: List[UtteranceEntity] = list(res.scalars().all())
        if not utterances:
            return {"doc_id": doc_id, "chunks_inserted": 0}

        # Existing chunk hashes to avoid dupes
        from sqlalchemy import select as sa_select

        ex_stmt = sa_select(ChunkEntity.chunk_hash).where(
            ChunkEntity.document_id == doc_id
        )
        ex_res = await session.execute(ex_stmt)
        existing_hashes: set[str] = set(ex_res.scalars().all())

        # Build contiguous section groups to preserve topical coherence
        groups: List[Tuple[str, List[UtteranceEntity]]] = []
        cur_section = None
        cur_group: List[UtteranceEntity] = []
        for u in utterances:
            sec = u.section or "other"
            if cur_section is None:
                cur_section = sec
            if sec != cur_section and cur_group:
                groups.append((cur_section, cur_group))
                cur_group = []
                cur_section = sec
            cur_group.append(u)
        if cur_group:
            groups.append((cur_section or "other", cur_group))

        seq = 0
        for section, group_utts in groups:
            # Concatenate "Speaker: text" blocks with paragraph spacing
            combined = []
            speakers_in_group = []
            utt_ids_in_group = []
            for u in group_utts:
                speech_norm = _normalize_text(u.speech or "")
                if not speech_norm:
                    continue
                combined.append(f"{u.speaker}: {speech_norm}")
                speakers_in_group.append(u.speaker)
                utt_ids_in_group.append(u.utterance_id)
            if not combined:
                continue
            text = "\n\n".join(combined)
            parts = splitter.split_text(text)
            for part in parts:
                content = part.strip()
                if not content:
                    continue
                content_norm = _normalize_text(content)
                token_count = count_tokens(content_norm)
                seq += 1
                h_src = f"{doc_id}|{seq}|{section}|{content_norm}"
                chash = hashlib.sha256(h_src.encode("utf-8")).hexdigest()
                if chash in existing_hashes:
                    continue
                chunk = ChunkEntity(
                    document_id=doc_id,
                    chunk_hash=chash,
                    chunk_type=ChunkTypeEnum.TEXT,
                    content=content,
                    content_normalized=content_norm,
                    page_number=None,
                    section_title=section,
                    sequence_number=seq,
                    token_count=token_count,
                    extra_metadata={
                        "speakers": list(dict.fromkeys(speakers_in_group)),
                        "utterance_ids": utt_ids_in_group,
                    },
                )
                session.add(chunk)
                inserted += 1

        await session.commit()

    return {"doc_id": doc_id, "chunks_inserted": inserted}
